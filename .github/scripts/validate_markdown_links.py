#!/usr/bin/env python3
import os
import re
import sys
import urllib.parse
from typing import Dict, List, Set, Tuple


EXCLUDE_DIRS = {".git", "node_modules", ".vs"}


LINK_RE = re.compile(r"(?<!\!)\[(?P<text>[^\]]+)\]\((?P<target>[^)\s]+)(?:\s+\"[^\"]*\")?\)")


def unescape_uri(s: str) -> str:
    try:
        return urllib.parse.unquote(s)
    except Exception:
        return s


def normalize_heading_to_id(text: str) -> str:
    """
    Approximate GitHub-style anchor slug generation (GFM):
    - Lowercase
    - Strip Markdown formatting and code ticks
    - Remove most punctuation
    - Replace whitespace with hyphens
    - Collapse multiple hyphens
    - Trim leading/trailing hyphens
    """
    if not text:
        return ""
    t = text
    # Remove inline code ticks
    t = t.replace("`", "")
    # Remove Markdown emphasis markers
    t = t.replace("*", "").replace("_", "").replace("~", "")
    # Remove link/image markup inside headings e.g. [text](url)
    t = re.sub(r"!?\[[^\]]*\]\([^)]*\)", "", t)
    # Remove HTML tags
    t = re.sub(r"<[^>]+>", "", t)
    # Lowercase
    t = t.lower()
    # Replace whitespace with hyphens
    t = re.sub(r"\s+", "-", t)
    # Remove punctuation except hyphens and alphanumerics
    t = re.sub(r"[^a-z0-9-]", "", t)
    # Collapse multiple hyphens
    t = re.sub(r"-+", "-", t)
    # Trim hyphens
    t = t.strip("-")
    return t


def collect_heading_ids(file_path: str) -> Set[str]:
    """
    Build the set of anchor IDs generated by headings within a markdown file.
    Handles ATX (# ...) and Setext (underlined) headings. Accounts for duplicate
    slugs by adding -1, -2, ... suffixes (GitHub behavior).
    """
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
    except Exception:
        return set()

    ids: Set[str] = set()
    slug_counts: Dict[str, int] = {}

    def add_slug_from_text(text: str):
        slug = normalize_heading_to_id(text)
        if slug == "":
            return
        count = slug_counts.get(slug, 0)
        if count == 0:
            final = slug
        else:
            final = f"{slug}-{count}"
        slug_counts[slug] = count + 1
        ids.add(final)

    # ATX headings
    atx_re = re.compile(r"^\s{0,3}#{1,6}\s+(.*)$")

    # Walk lines, handle setext headings by looking ahead
    i = 0
    while i < len(lines):
        line = lines[i].rstrip("\n")
        m = atx_re.match(line)
        if m:
            add_slug_from_text(m.group(1).strip())
            i += 1
            continue
        # Setext H1/H2
        if i + 1 < len(lines):
            underline = lines[i + 1].rstrip("\n")
            if re.match(r"^\s{0,3}=+\s*$", underline) or re.match(r"^\s{0,3}-+\s*$", underline):
                add_slug_from_text(line.strip())
                i += 2
                continue
        i += 1

    return ids


def is_external(target: str) -> bool:
    return bool(re.match(r"^(https?://|mailto:|tel:|data:)", target))


def resolve_path(base_dir: str, target_path: str) -> str:
    return os.path.normpath(os.path.join(base_dir, target_path))


def check_internal_link(src_file: str, target_raw: str) -> Tuple[bool, str]:
    # Separate fragment
    if target_raw.startswith("#"):
        # Anchor within same file
        frag = target_raw[1:]
        anchor = unescape_uri(frag)
        anchor = anchor.strip()
        anchor = anchor.lower()
        ids = collect_heading_ids(src_file)
        if anchor in ids:
            return True, ""
        return False, f"dangling anchor '#{frag}' (no matching heading)"

    # Split off query/fragment
    core = re.sub(r"[?#].*$", "", target_raw)
    core = unescape_uri(core)
    base_dir = os.path.dirname(src_file)
    target_fs = resolve_path(base_dir, core)
    if not os.path.exists(target_fs):
        return False, f"target file not found: {core}"

    # Fragment check if present
    m = re.search(r"#(.+)$", target_raw)
    if m:
        frag = m.group(1)
        anchor = unescape_uri(frag).strip().lower()
        ids = collect_heading_ids(target_fs)
        if anchor not in ids:
            return False, f"dangling anchor '#{frag}' in {core}"

    return True, ""


def main(paths: List[str]) -> int:
    issues = 0

    def iter_markdown_files() -> List[str]:
        files: List[str] = []
        for p in paths:
            if os.path.isdir(p):
                for dirpath, dirnames, filenames in os.walk(p):
                    dirnames[:] = [d for d in dirnames if d not in EXCLUDE_DIRS]
                    for filename in filenames:
                        if filename.lower().endswith(".md"):
                            files.append(os.path.join(dirpath, filename))
            else:
                if p.lower().endswith(".md") and os.path.exists(p):
                    files.append(p)
        return files

    files = iter_markdown_files()
    for path in files:
        try:
            with open(path, "r", encoding="utf-8") as f:
                lines = f.readlines()
        except Exception:
            continue
        for idx, line in enumerate(lines, start=1):
            for m in LINK_RE.finditer(line):
                target = m.group("target").strip()
                # Skip images, external links, anchors we can't resolve externally here
                if is_external(target):
                    continue
                ok, reason = check_internal_link(path, target)
                if not ok:
                    issues += 1
                    print(f"{path}:{idx}: Broken link '{target}': {reason}")

    if issues:
        print(f"Found {issues} broken internal markdown link(s).", file=sys.stderr)
        print("Fix the paths or anchors so links resolve.", file=sys.stderr)
        return 1
    return 0


if __name__ == "__main__":
    args = sys.argv[1:]
    if not args:
        args = ["."]
    sys.exit(main(args))
